"""
Pydantic schemas for question generation run tracking endpoints.

These schemas support the admin API for tracking and analyzing
question generation service execution metrics.
"""
from pydantic import BaseModel, Field
from typing import Optional, Dict, Any, List
from datetime import datetime
from enum import Enum


class GenerationRunStatusSchema(str, Enum):
    """Status enumeration for question generation runs."""

    RUNNING = "running"
    SUCCESS = "success"
    PARTIAL_FAILURE = "partial_failure"
    FAILED = "failed"


class ProviderMetrics(BaseModel):
    """Schema for per-provider metrics breakdown."""

    generated: int = Field(
        default=0, description="Questions generated by this provider"
    )
    api_calls: int = Field(default=0, description="API calls made to this provider")
    failures: int = Field(default=0, description="Failures from this provider")


class ErrorSummary(BaseModel):
    """Schema for error tracking summary."""

    by_category: Dict[str, int] = Field(
        default_factory=dict,
        description="Error counts by category (e.g., rate_limit, api_error)",
    )
    by_severity: Dict[str, int] = Field(
        default_factory=dict,
        description="Error counts by severity (e.g., high, medium, low)",
    )
    critical_count: int = Field(default=0, description="Count of critical errors")


class QuestionGenerationRunCreate(BaseModel):
    """
    Schema for creating a question generation run record.

    Used by the question-service to report execution metrics to the backend.
    Supports both initial "running" status reports and final completion reports.
    """

    # Execution timing
    started_at: datetime = Field(..., description="When the generation run started")
    completed_at: Optional[datetime] = Field(
        None, description="When the generation run completed (null if still running)"
    )
    duration_seconds: Optional[float] = Field(
        None, description="Total duration in seconds (null if still running)"
    )

    # Status & outcome
    status: GenerationRunStatusSchema = Field(
        ..., description="Run status: running, success, partial_failure, or failed"
    )
    exit_code: Optional[int] = Field(
        None, description="Exit code (0-6 matching run_generation.py codes)"
    )

    # Generation metrics
    questions_requested: int = Field(..., description="Number of questions requested")
    questions_generated: int = Field(
        default=0, description="Number of questions generated"
    )
    generation_failures: int = Field(
        default=0, description="Number of generation failures"
    )
    generation_success_rate: Optional[float] = Field(
        None, description="Generation success rate (0.0-1.0)"
    )

    # Evaluation metrics
    questions_evaluated: int = Field(
        default=0, description="Questions evaluated by arbiter"
    )
    questions_approved: int = Field(
        default=0, description="Questions approved by arbiter"
    )
    questions_rejected: int = Field(
        default=0, description="Questions rejected by arbiter"
    )
    approval_rate: Optional[float] = Field(None, description="Approval rate (0.0-1.0)")
    avg_arbiter_score: Optional[float] = Field(
        None, description="Average arbiter score"
    )
    min_arbiter_score: Optional[float] = Field(
        None, description="Minimum arbiter score"
    )
    max_arbiter_score: Optional[float] = Field(
        None, description="Maximum arbiter score"
    )

    # Deduplication metrics
    duplicates_found: int = Field(default=0, description="Total duplicates found")
    exact_duplicates: int = Field(default=0, description="Exact text duplicates found")
    semantic_duplicates: int = Field(default=0, description="Semantic duplicates found")
    duplicate_rate: Optional[float] = Field(
        None, description="Duplicate rate (0.0-1.0)"
    )

    # Database metrics
    questions_inserted: int = Field(
        default=0, description="Questions inserted into database"
    )
    insertion_failures: int = Field(
        default=0, description="Database insertion failures"
    )

    # Overall success
    overall_success_rate: Optional[float] = Field(
        None,
        description="Overall success rate (questions_inserted / questions_requested)",
    )
    total_errors: int = Field(default=0, description="Total errors encountered")

    # API usage
    total_api_calls: int = Field(
        default=0, description="Total API calls across all providers"
    )

    # Breakdown by provider (flexible JSON)
    provider_metrics: Optional[Dict[str, Dict[str, Any]]] = Field(
        None,
        description='Per-provider metrics breakdown. Example: {"openai": {"generated": 10, "api_calls": 15}}',
    )

    # Breakdown by question type (flexible JSON)
    type_metrics: Optional[Dict[str, int]] = Field(
        None,
        description='Questions generated per type. Example: {"pattern_recognition": 8, "logical_reasoning": 12}',
    )

    # Breakdown by difficulty (flexible JSON)
    difficulty_metrics: Optional[Dict[str, int]] = Field(
        None,
        description='Questions generated per difficulty. Example: {"easy": 15, "medium": 22, "hard": 13}',
    )

    # Error tracking (flexible JSON)
    error_summary: Optional[Dict[str, Any]] = Field(
        None,
        description='Error breakdown. Example: {"by_category": {"rate_limit": 2}, "critical_count": 0}',
    )

    # Configuration used
    prompt_version: Optional[str] = Field(
        None, max_length=50, description="Prompt version used"
    )
    arbiter_config_version: Optional[str] = Field(
        None, max_length=50, description="Arbiter config version used"
    )
    min_arbiter_score_threshold: Optional[float] = Field(
        None, description="Minimum arbiter score threshold for approval"
    )

    # Environment context
    environment: Optional[str] = Field(
        None,
        max_length=20,
        description="Environment: production, staging, or development",
    )
    triggered_by: Optional[str] = Field(
        None, max_length=50, description="Trigger source: scheduler, manual, or webhook"
    )


class QuestionGenerationRunRead(BaseModel):
    """
    Schema for reading a question generation run record.

    Returns full details of a generation run including all metrics
    and JSONB breakdown fields.
    """

    # Identity
    id: int = Field(..., description="Run ID")

    # Execution timing
    started_at: datetime = Field(..., description="When the generation run started")
    completed_at: Optional[datetime] = Field(
        None, description="When the generation run completed"
    )
    duration_seconds: Optional[float] = Field(
        None, description="Total duration in seconds"
    )

    # Status & outcome
    status: GenerationRunStatusSchema = Field(..., description="Run status")
    exit_code: Optional[int] = Field(None, description="Exit code")

    # Generation metrics
    questions_requested: int = Field(..., description="Number of questions requested")
    questions_generated: int = Field(..., description="Number of questions generated")
    generation_failures: int = Field(..., description="Number of generation failures")
    generation_success_rate: Optional[float] = Field(
        None, description="Generation success rate"
    )

    # Evaluation metrics
    questions_evaluated: int = Field(..., description="Questions evaluated by arbiter")
    questions_approved: int = Field(..., description="Questions approved by arbiter")
    questions_rejected: int = Field(..., description="Questions rejected by arbiter")
    approval_rate: Optional[float] = Field(None, description="Approval rate")
    avg_arbiter_score: Optional[float] = Field(
        None, description="Average arbiter score"
    )
    min_arbiter_score: Optional[float] = Field(
        None, description="Minimum arbiter score"
    )
    max_arbiter_score: Optional[float] = Field(
        None, description="Maximum arbiter score"
    )

    # Deduplication metrics
    duplicates_found: int = Field(..., description="Total duplicates found")
    exact_duplicates: int = Field(..., description="Exact text duplicates found")
    semantic_duplicates: int = Field(..., description="Semantic duplicates found")
    duplicate_rate: Optional[float] = Field(None, description="Duplicate rate")

    # Database metrics
    questions_inserted: int = Field(..., description="Questions inserted into database")
    insertion_failures: int = Field(..., description="Database insertion failures")

    # Overall success
    overall_success_rate: Optional[float] = Field(
        None, description="Overall success rate"
    )
    total_errors: int = Field(..., description="Total errors encountered")

    # API usage
    total_api_calls: int = Field(..., description="Total API calls")

    # Breakdown fields (JSONB)
    provider_metrics: Optional[Dict[str, Dict[str, Any]]] = Field(
        None, description="Per-provider metrics breakdown"
    )
    type_metrics: Optional[Dict[str, int]] = Field(
        None, description="Questions generated per type"
    )
    difficulty_metrics: Optional[Dict[str, int]] = Field(
        None, description="Questions generated per difficulty"
    )
    error_summary: Optional[Dict[str, Any]] = Field(None, description="Error breakdown")

    # Configuration used
    prompt_version: Optional[str] = Field(None, description="Prompt version used")
    arbiter_config_version: Optional[str] = Field(
        None, description="Arbiter config version used"
    )
    min_arbiter_score_threshold: Optional[float] = Field(
        None, description="Minimum arbiter score threshold"
    )

    # Environment context
    environment: Optional[str] = Field(None, description="Environment")
    triggered_by: Optional[str] = Field(None, description="Trigger source")

    # Metadata
    created_at: datetime = Field(..., description="Record creation timestamp")

    class Config:
        """Pydantic configuration."""

        from_attributes = True


class QuestionGenerationRunSummary(BaseModel):
    """
    Summary schema for listing generation runs.

    Returns essential fields without the full JSONB breakdowns
    to optimize list queries.
    """

    id: int = Field(..., description="Run ID")
    started_at: datetime = Field(..., description="When the run started")
    completed_at: Optional[datetime] = Field(None, description="When the run completed")
    duration_seconds: Optional[float] = Field(None, description="Duration in seconds")
    status: GenerationRunStatusSchema = Field(..., description="Run status")
    questions_requested: int = Field(..., description="Questions requested")
    questions_inserted: int = Field(..., description="Questions inserted")
    overall_success_rate: Optional[float] = Field(
        None, description="Overall success rate"
    )
    approval_rate: Optional[float] = Field(None, description="Approval rate")
    avg_arbiter_score: Optional[float] = Field(
        None, description="Average arbiter score"
    )
    total_errors: int = Field(..., description="Total errors")
    environment: Optional[str] = Field(None, description="Environment")
    triggered_by: Optional[str] = Field(None, description="Trigger source")

    class Config:
        """Pydantic configuration."""

        from_attributes = True


class QuestionGenerationRunListResponse(BaseModel):
    """
    Schema for paginated list of generation runs.
    """

    runs: List[QuestionGenerationRunSummary] = Field(
        ..., description="List of generation run summaries"
    )
    total: int = Field(..., description="Total number of runs matching filters")
    page: int = Field(..., description="Current page number (1-indexed)")
    page_size: int = Field(..., description="Number of items per page")
    total_pages: int = Field(..., description="Total number of pages")


class QuestionGenerationRunStats(BaseModel):
    """
    Schema for aggregated statistics over a time period.

    Provides summary metrics for trend analysis, provider comparison,
    and quality monitoring dashboards.
    """

    # Time period
    period_start: datetime = Field(..., description="Start of the analysis period")
    period_end: datetime = Field(..., description="End of the analysis period")
    total_runs: int = Field(..., description="Total number of runs in period")

    # Run outcomes
    successful_runs: int = Field(..., description="Runs with status 'success'")
    failed_runs: int = Field(..., description="Runs with status 'failed'")
    partial_failure_runs: int = Field(
        ..., description="Runs with status 'partial_failure'"
    )

    # Aggregate generation metrics
    total_questions_requested: int = Field(..., description="Total questions requested")
    total_questions_generated: int = Field(..., description="Total questions generated")
    total_questions_inserted: int = Field(..., description="Total questions inserted")
    avg_overall_success_rate: Optional[float] = Field(
        None, description="Average overall success rate"
    )

    # Aggregate evaluation metrics
    avg_approval_rate: Optional[float] = Field(
        None, description="Average approval rate"
    )
    avg_arbiter_score: Optional[float] = Field(
        None, description="Average arbiter score"
    )
    min_arbiter_score: Optional[float] = Field(
        None, description="Minimum arbiter score across all runs"
    )
    max_arbiter_score: Optional[float] = Field(
        None, description="Maximum arbiter score across all runs"
    )

    # Aggregate deduplication metrics
    total_duplicates_found: int = Field(..., description="Total duplicates found")
    avg_duplicate_rate: Optional[float] = Field(
        None, description="Average duplicate rate"
    )

    # Performance metrics
    avg_duration_seconds: Optional[float] = Field(
        None, description="Average run duration"
    )
    total_api_calls: int = Field(..., description="Total API calls")
    avg_api_calls_per_question: Optional[float] = Field(
        None, description="Average API calls per successfully inserted question"
    )

    # Error summary
    total_errors: int = Field(..., description="Total errors across all runs")

    # Provider comparison (aggregated from JSONB)
    provider_summary: Optional[Dict[str, Dict[str, Any]]] = Field(
        None,
        description="Aggregated metrics per provider. Example: {'openai': {'total_generated': 100, 'avg_success_rate': 0.85}}",
    )

    # Trend indicators
    success_rate_trend: Optional[str] = Field(
        None, description="Trend direction: improving, declining, or stable"
    )
    approval_rate_trend: Optional[str] = Field(
        None, description="Trend direction: improving, declining, or stable"
    )
