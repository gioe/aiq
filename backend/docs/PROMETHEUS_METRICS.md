# Prometheus Metrics

The AIQ backend exposes Prometheus-compatible metrics at `/v1/metrics` for monitoring and observability.

## Quick Start

### 1. Enable Metrics

Add to your `.env` file:

```bash
# Enable OpenTelemetry metrics
OTEL_ENABLED=true
OTEL_METRICS_ENABLED=true

# Enable Prometheus metrics endpoint
PROMETHEUS_METRICS_ENABLED=true
```

### 2. Access Metrics

```bash
curl http://localhost:8000/v1/metrics
```

### 3. Configure Prometheus

Add to your `prometheus.yml`:

```yaml
scrape_configs:
  - job_name: 'aiq-backend'
    scrape_interval: 15s
    static_configs:
      - targets: ['localhost:8000']
    metrics_path: '/v1/metrics'
```

## Available Metrics

### HTTP Metrics

These metrics track all HTTP requests to the backend API.

| Metric | Type | Description | Labels |
|--------|------|-------------|--------|
| `http_server_requests` | Counter | Total HTTP requests | `http.method`, `http.route`, `http.status_code` |
| `http_server_request_duration` | Histogram | HTTP request latency (seconds) | `http.method`, `http.route`, `http.status_code` |

**Example queries:**

```promql
# Request rate per endpoint
rate(http_server_requests[5m])

# 95th percentile request latency
histogram_quantile(0.95, rate(http_server_request_duration_bucket[5m]))

# Error rate (4xx and 5xx)
rate(http_server_requests{http_status_code=~"4..|5.."}[5m])
```

### Database Metrics

Track database query performance.

| Metric | Type | Description | Labels |
|--------|------|-------------|--------|
| `db_query_duration` | Histogram | Database query duration (seconds) | `db.operation`, `db.table` |

**Example queries:**

```promql
# Slowest database queries by table
histogram_quantile(0.99, rate(db_query_duration_bucket[5m])) by (db_table)

# Query rate by operation type
rate(db_query_duration_count[5m]) by (db_operation)
```

### Test Session Metrics

Track cognitive test sessions throughout their lifecycle.

| Metric | Type | Description | Labels |
|--------|------|-------------|--------|
| `test_sessions_active` | Gauge | Current number of in-progress test sessions | - |
| `test_sessions_started` | Counter | Total test sessions started | `test.adaptive`, `test.question_count` |
| `test_sessions_completed` | Counter | Total test sessions completed | `test.adaptive`, `test.question_count` |
| `test_sessions_abandoned` | Counter | Total test sessions abandoned | `test.adaptive`, `test.questions_answered` |

**Example queries:**

```promql
# Active test sessions right now
test_sessions_active

# Test completion rate (last hour)
rate(test_sessions_completed[1h]) / rate(test_sessions_started[1h])

# Abandonment rate by adaptive vs fixed-form
rate(test_sessions_abandoned[1h]) by (test_adaptive)

# Average questions answered before abandonment
avg(test_sessions_abandoned{test_questions_answered!=""}) by (test_adaptive)
```

### Question Metrics

Track AI-generated questions and question serving.

| Metric | Type | Description | Labels |
|--------|------|-------------|--------|
| `questions_generated` | Counter | Total questions generated by AI | `question.type`, `question.difficulty` |
| `questions_served` | Counter | Total questions served to users | `test.adaptive` |

**Example queries:**

```promql
# Question generation rate by type
rate(questions_generated[1h]) by (question_type)

# Question distribution by difficulty
sum(questions_generated) by (question_difficulty)

# Questions served per test session
rate(questions_served[1h]) / rate(test_sessions_started[1h])
```

### User Metrics

Track user lifecycle events.

| Metric | Type | Description | Labels |
|--------|------|-------------|--------|
| `users_registrations` | Counter | Total user registrations | - |

**Example queries:**

```promql
# New user registrations per day
increase(users_registrations[24h])

# Weekly registration trend
sum_over_time(increase(users_registrations[24h])[7d:24h])
```

### Error Metrics

Track application errors and exceptions.

| Metric | Type | Description | Labels |
|--------|------|-------------|--------|
| `app_errors` | Counter | Application errors | `error.type`, `http.route` |

**Example queries:**

```promql
# Error rate by endpoint
rate(app_errors[5m]) by (http_route)

# Most common error types
topk(5, rate(app_errors[1h]) by (error_type))

# Error percentage of total requests
rate(app_errors[5m]) / rate(http_server_requests[5m]) * 100
```

## Security

The `/v1/metrics` endpoint is intentionally **unauthenticated** to allow standard Prometheus scrapers to collect metrics without custom authentication configuration.

### Security Model

```
┌─────────────────────────────────────────────────────────────────────┐
│                       Railway Project                               │
│                                                                     │
│  ┌─────────────────┐        Internal        ┌─────────────────┐   │
│  │  AIQ Backend    │◄──────────────────────►│  Prometheus     │   │
│  │  /v1/metrics    │    (preferred path)     │  (scraper)      │   │
│  └─────────────────┘                         └─────────────────┘   │
│           │                                                         │
└───────────┼─────────────────────────────────────────────────────────┘
            │
            │ Endpoint accessible but:
            │ • No PII in metrics
            │ • Hidden from API docs
            │ • Only operational data
            ▼
┌─────────────────────────────┐
│   Public Internet           │
│   (low-value target)        │
└─────────────────────────────┘
```

### Railway Deployment (Production)

In Railway deployments, the `/v1/metrics` endpoint security relies on multiple layers:

1. **No sensitive data exposed**: The metrics endpoint contains only operational data (request counts, latencies, error rates). No PII or business-sensitive data is included in metric labels.

2. **Hidden from API documentation**: The endpoint has `include_in_schema=False`, so it doesn't appear in OpenAPI docs or API explorers.

3. **Internal scraper access**: A Prometheus scraper within the Railway project can access metrics via the internal hostname (e.g., `aiq-backend.railway.internal:8000`), avoiding public network traversal.

4. **Defense in depth**: While the endpoint is technically accessible via the public domain, the combination of non-sensitive data and obscurity provides acceptable risk for operational metrics.

> **Note**: If your metrics contain business-sensitive information, consider enabling Railway's Private Networking feature to restrict `/v1/metrics` to internal access only, or add application-level authentication (see [Adding Authentication](#alternative-adding-authentication) below).

### Data Security

The metrics endpoint is designed with data security in mind:

| Concern | Mitigation |
|---------|------------|
| **No PII in metrics** | User IDs, emails, and session IDs are never included in metric labels |
| **Bounded cardinality** | Labels use finite sets (e.g., HTTP methods, routes, status codes) |
| **Excluded from API docs** | The endpoint has `include_in_schema=False` to hide it from OpenAPI |
| **No sensitive business data** | Metrics are operational (latency, counts) not business-specific |

### Alternative: Adding Authentication

For deployments where network isolation isn't available (self-hosted, multi-cloud), you can add authentication:

**Option 1: Bearer Token (recommended)**

1. Add `METRICS_AUTH_TOKEN` to `app/core/config.py`:
   ```python
   METRICS_AUTH_TOKEN: str | None = None
   ```

2. Modify `app/api/v1/metrics.py` to require a token:
   ```python
   from fastapi import Header, HTTPException

   async def verify_metrics_token(x_metrics_token: str = Header(...)):
       if x_metrics_token != settings.METRICS_AUTH_TOKEN:
           raise HTTPException(status_code=401, detail="Invalid metrics token")

   @router.get("/metrics", dependencies=[Depends(verify_metrics_token)])
   async def prometheus_metrics():
       # ... existing implementation
   ```

3. Configure Prometheus:
```yaml
scrape_configs:
  - job_name: 'aiq-backend'
    bearer_token: 'your-metrics-token'  # pragma: allowlist secret
    # ... rest of config
```

**Option 2: Basic Authentication**

```yaml
scrape_configs:
  - job_name: 'aiq-backend'
    basic_auth:
      username: 'metrics'
      password: 'your-password'  # pragma: allowlist secret
    # ... rest of config
```

### Best Practices

1. **Prefer network isolation** over application-level auth when available (simpler, no token management)
2. **Use TLS** for metrics scraping in production (`scheme: https`)
3. **Monitor cardinality** to prevent metric explosion from unbounded labels
4. **Rotate tokens** periodically if using authentication
5. **Audit access** via scraper logs if security is a concern

## Prometheus Configuration

### Basic Scraping

```yaml
# prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  - job_name: 'aiq-backend'
    scrape_interval: 15s
    static_configs:
      - targets: ['localhost:8000']
    metrics_path: '/v1/metrics'
```

### Production Configuration

```yaml
# prometheus.yml
scrape_configs:
  - job_name: 'aiq-backend-production'
    scrape_interval: 15s
    scrape_timeout: 10s
    static_configs:
      - targets: ['aiq-backend-production.up.railway.app:443']
    metrics_path: '/v1/metrics'
    scheme: https
    # Optional: Add service discovery for dynamic environments
    # relabel_configs:
    #   - source_labels: [__address__]
    #     target_label: instance
    #     replacement: 'aiq-backend-production'
```

### With Authentication

If your metrics endpoint requires authentication:

```yaml
scrape_configs:
  - job_name: 'aiq-backend'
    scrape_interval: 15s
    static_configs:
      - targets: ['localhost:8000']
    metrics_path: '/v1/metrics'
    bearer_token: 'your-metrics-token'
    # OR use basic auth:
    # basic_auth:
    #   username: 'metrics'
    #   password: 'your-password'  # pragma: allowlist secret
```

## Grafana Integration

### Add Prometheus Data Source

1. In Grafana, go to **Configuration** → **Data Sources**
2. Click **Add data source**
3. Select **Prometheus**
4. Set URL to your Prometheus server (e.g., `http://prometheus:9090`)
5. Click **Save & Test**

### Import Dashboard

Use the example dashboard configuration below or create your own.

### Example Dashboard

```json
{
  "dashboard": {
    "title": "AIQ Backend Metrics",
    "panels": [
      {
        "title": "Request Rate",
        "targets": [
          {
            "expr": "rate(http_server_requests[5m])",
            "legendFormat": "{{http_method}} {{http_route}}"
          }
        ],
        "type": "graph"
      },
      {
        "title": "Request Latency (p95)",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(http_server_request_duration_bucket[5m]))",
            "legendFormat": "{{http_route}}"
          }
        ],
        "type": "graph"
      },
      {
        "title": "Active Test Sessions",
        "targets": [
          {
            "expr": "test_sessions_active",
            "legendFormat": "Active Sessions"
          }
        ],
        "type": "stat"
      },
      {
        "title": "Test Completion Rate",
        "targets": [
          {
            "expr": "rate(test_sessions_completed[1h]) / rate(test_sessions_started[1h]) * 100",
            "legendFormat": "Completion Rate %"
          }
        ],
        "type": "gauge"
      },
      {
        "title": "Error Rate",
        "targets": [
          {
            "expr": "rate(app_errors[5m])",
            "legendFormat": "{{error_type}}"
          }
        ],
        "type": "graph"
      }
    ]
  }
}
```

## Example Grafana Dashboard JSON

Save this to a file (e.g., `aiq-dashboard.json`) and import into Grafana:

```json
{
  "annotations": {
    "list": []
  },
  "editable": true,
  "fiscalYearStartMonth": 0,
  "graphTooltip": 0,
  "id": null,
  "links": [],
  "liveNow": false,
  "panels": [
    {
      "datasource": {
        "type": "prometheus",
        "uid": "${DS_PROMETHEUS}"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "drawStyle": "line",
            "fillOpacity": 10,
            "gradientMode": "none",
            "hideFrom": {
              "tooltip": false,
              "viz": false,
              "legend": false
            },
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "never",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "reqps"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 0
      },
      "id": 1,
      "options": {
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "single",
          "sort": "none"
        }
      },
      "targets": [
        {
          "expr": "rate(http_server_requests[5m])",
          "legendFormat": "{{http_method}} {{http_route}}",
          "refId": "A"
        }
      ],
      "title": "Request Rate",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "${DS_PROMETHEUS}"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "thresholds"
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              },
              {
                "color": "yellow",
                "value": 10
              },
              {
                "color": "red",
                "value": 50
              }
            ]
          },
          "unit": "short"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 0
      },
      "id": 2,
      "options": {
        "orientation": "auto",
        "reduceOptions": {
          "values": false,
          "calcs": [
            "lastNotNull"
          ],
          "fields": ""
        },
        "showThresholdLabels": false,
        "showThresholdMarkers": true
      },
      "targets": [
        {
          "expr": "test_sessions_active",
          "legendFormat": "Active Sessions",
          "refId": "A"
        }
      ],
      "title": "Active Test Sessions",
      "type": "gauge"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "${DS_PROMETHEUS}"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "drawStyle": "line",
            "fillOpacity": 10,
            "gradientMode": "none",
            "hideFrom": {
              "tooltip": false,
              "viz": false,
              "legend": false
            },
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "never",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              },
              {
                "color": "red",
                "value": 1
              }
            ]
          },
          "unit": "s"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 8
      },
      "id": 3,
      "options": {
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "single",
          "sort": "none"
        }
      },
      "targets": [
        {
          "expr": "histogram_quantile(0.95, rate(http_server_request_duration_bucket[5m]))",
          "legendFormat": "p95 {{http_route}}",
          "refId": "A"
        }
      ],
      "title": "Request Latency (p95)",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "${DS_PROMETHEUS}"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "drawStyle": "line",
            "fillOpacity": 10,
            "gradientMode": "none",
            "hideFrom": {
              "tooltip": false,
              "viz": false,
              "legend": false
            },
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "never",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              },
              {
                "color": "red",
                "value": 0.01
              }
            ]
          },
          "unit": "percentunit"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 8
      },
      "id": 4,
      "options": {
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "single",
          "sort": "none"
        }
      },
      "targets": [
        {
          "expr": "rate(test_sessions_completed[1h]) / rate(test_sessions_started[1h])",
          "legendFormat": "Completion Rate",
          "refId": "A"
        }
      ],
      "title": "Test Completion Rate",
      "type": "timeseries"
    }
  ],
  "refresh": "10s",
  "schemaVersion": 38,
  "style": "dark",
  "tags": ["aiq", "backend"],
  "templating": {
    "list": []
  },
  "time": {
    "from": "now-6h",
    "to": "now"
  },
  "timepicker": {},
  "timezone": "",
  "title": "AIQ Backend Metrics",
  "uid": "aiq-backend",
  "version": 1,
  "weekStart": ""
}
```

## Alerting Rules

Create alerting rules in Prometheus for critical metrics:

```yaml
# prometheus-rules.yml
groups:
  - name: aiq-backend-alerts
    interval: 30s
    rules:
      # High error rate
      - alert: HighErrorRate
        expr: rate(app_errors[5m]) > 0.1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value }} errors/sec"

      # High latency
      - alert: HighLatency
        expr: histogram_quantile(0.95, rate(http_server_request_duration_bucket[5m])) > 1.0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High request latency detected"
          description: "95th percentile latency is {{ $value }}s"

      # Low test completion rate
      - alert: LowTestCompletionRate
        expr: rate(test_sessions_completed[1h]) / rate(test_sessions_started[1h]) < 0.7
        for: 30m
        labels:
          severity: warning
        annotations:
          summary: "Test completion rate is low"
          description: "Completion rate is {{ $value | humanizePercentage }}"

      # Many abandoned tests
      - alert: HighAbandonmentRate
        expr: rate(test_sessions_abandoned[1h]) / rate(test_sessions_started[1h]) > 0.3
        for: 30m
        labels:
          severity: warning
        annotations:
          summary: "High test abandonment rate"
          description: "Abandonment rate is {{ $value | humanizePercentage }}"
```

## Troubleshooting

### Metrics Endpoint Returns 503

**Cause:** Metrics are not enabled or OpenTelemetry failed to initialize.

**Solution:**

1. Check `.env` configuration:
   ```bash
   OTEL_ENABLED=true
   OTEL_METRICS_ENABLED=true
   PROMETHEUS_METRICS_ENABLED=true
   ```

2. Check application logs for initialization errors:
   ```bash
   grep -i "metrics" logs/app.log
   ```

3. Verify packages are installed:
   ```bash
   pip list | grep -E "opentelemetry|prometheus"
   ```

### No Metrics Showing Up

**Cause:** Prometheus exporter not initialized or no activity yet.

**Solution:**

1. Generate some traffic to the backend
2. Wait for the metrics export interval (60 seconds by default)
3. Check if the exporter is initialized:
   ```bash
   curl http://localhost:8000/v1/metrics | grep -c "http_server_requests"
   ```

### Metrics Seem Stale

**Cause:** Export interval is too long or scrape interval mismatch.

**Solution:**

1. Reduce export interval in `.env`:
   ```bash
   OTEL_METRICS_EXPORT_INTERVAL_MILLIS=15000  # 15 seconds
   ```

2. Ensure Prometheus scrape interval matches or is less than export interval

## Best Practices

### Metric Naming

- Use underscores for word separation (e.g., `http_server_requests`)
- Include units in the name when ambiguous (e.g., `duration_seconds`)
- Use consistent prefixes by domain (e.g., `test_`, `http_`, `db_`)

### Labels

- Keep cardinality low (avoid high-cardinality labels like user IDs)
- Use labels for dimensions you want to filter/group by
- Prefer fewer labels with meaningful values

### Retention

- Prometheus default retention is 15 days
- For production, configure longer retention:
  ```bash
  prometheus --storage.tsdb.retention.time=90d
  ```

### Performance

- Monitor Prometheus memory usage as metric count grows
- Use recording rules for expensive queries
- Consider using remote storage for long-term retention

## Metric Cardinality

Cardinality refers to the number of unique time series created by a metric. High cardinality can cause performance issues with Prometheus storage and querying.

### Current Cardinality Estimates

| Metric | Labels | Estimated Cardinality |
|--------|--------|----------------------|
| `http_server_requests` | method (5) × route (~20) × status_code (5) | ~500 |
| `http_server_request_duration` | method (5) × route (~20) × status_code (5) | ~500 |
| `test_sessions_started` | adaptive (2) × question_count (~5) | ~10 |
| `test_sessions_completed` | adaptive (2) × question_count (~5) | ~10 |
| `test_sessions_abandoned` | adaptive (2) × questions_answered (~25) | ~50 |
| `questions_generated` | type (~6) × difficulty (3) | ~18 |
| `questions_served` | adaptive (2) | ~2 |
| `app_errors` | type (~10) × route (~20) | ~200 |
| `user_registrations` | (none) | 1 |

**Total estimated cardinality**: ~1,300 series

This is well within Prometheus's comfortable operating range (< 100K series for a single instance).

### Cardinality Best Practices

1. **Avoid high-cardinality labels**: Never use user IDs, session IDs, request IDs, or timestamps as label values
2. **Bound label cardinality**: Ensure labels have a finite, predictable set of values
3. **Use histograms over summaries**: Histograms are more aggregation-friendly
4. **Monitor your cardinality**: Use `prometheus_tsdb_head_series` to track series count

### Adding New Metrics

When adding new metrics, consider:

```yaml
# Good: Bounded cardinality
question.type: ["pattern", "logic", "verbal", "spatial", "memory", "arithmetic"]
question.difficulty: ["easy", "medium", "hard"]

# Bad: Unbounded cardinality (DO NOT USE)
user.id: "uuid-..."        # Unique per user
session.id: "uuid-..."     # Unique per session
timestamp: "2024-..."      # Continuous values
```

### Multi-Worker Considerations

The `test_sessions_active` gauge tracks sessions using delta calculations. In multi-worker deployments (e.g., Gunicorn with multiple workers):

- Each worker maintains its own internal counter
- Process restarts reset the internal counter, causing drift
- For accurate active session counts, query the database directly:

```promql
# This may be inaccurate in multi-worker deployments
test_sessions_active

# Alternative: Use an external query or custom exporter
# that queries the database for active session count
```

## Further Reading

- [Prometheus Documentation](https://prometheus.io/docs/)
- [Grafana Dashboards](https://grafana.com/docs/grafana/latest/dashboards/)
- [OpenTelemetry Metrics](https://opentelemetry.io/docs/concepts/signals/metrics/)
- [Prometheus Best Practices](https://prometheus.io/docs/practices/naming/)
