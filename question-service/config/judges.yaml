# Judge Model Configuration
# Maps question types to specific judge models for quality evaluation
#
# Each question type can be evaluated by a different model based on
# benchmark performance and domain expertise.
#
# Configuration Structure:
#   question_type:
#     model: Exact API model identifier (must match provider SDK expectations)
#     provider: Provider name ("openai", "anthropic", "google", "xai")
#     rationale: Explanation of why this model was chosen
#     enabled: Whether this judge is active (default: true)
#     fallback: Optional fallback provider if primary is unavailable
#     fallback_model: Optional specific model to use with the fallback provider
#
# IMPORTANT - Model Naming Convention:
#   Model identifiers MUST match the exact strings used by each provider's API:
#   - Anthropic: claude-opus-4-5-20251101, claude-sonnet-4-5-20250929, etc.
#   - Google: gemini-3-pro-preview, gemini-3-flash-preview, gemini-2.5-pro, etc.
#   - OpenAI: gpt-5.2, gpt-4-turbo, o4-mini, etc.
#   - xAI: grok-4, grok-3, grok-beta
#   See each provider's get_available_models() for the full list.
#
# Google Model Naming (Preview vs Stable):
#   Google uses "-preview" suffix for new models in preview phase (e.g., gemini-3-pro-preview).
#   Once a model graduates to stable, the suffix is removed (e.g., gemini-2.5-pro).
#   Preview models may have API changes; stable models have frozen APIs.
#   Always check Google's release notes when updating model identifiers.

version: "1.0"

judges:
  math:
    model: "grok-4"
    provider: "xai"
    rationale: "Exceptional math performance: GSM8K (95.2%), AIME 2024 (100%), USAMO 2025 (61.9%). Grok 4 demonstrates world-class mathematical reasoning with perfect scores on advanced math competitions."
    benchmark_data:
      GSM8K: "95.2%"
      AIME_2024: "100%"
      USAMO_2025: "61.9%"
      MMLU: "92.1%"
    enabled: true
    fallback: "openai"
    fallback_model: "gpt-5.2"
    # Decision Log (Jan 2026): Evaluated Grok 4 Heavy as alternative.
    # Grok 4 Heavy achieves identical USAMO performance (61.9%) but is NOT available
    # via API - only through SuperGrok Heavy subscription ($300/month). The multi-agent
    # architecture adds 4-7x latency overhead with no meaningful improvement for our
    # math judging use case. Keeping grok-4 as optimal choice for API-accessible math judging.

  logic:
    model: "claude-sonnet-4-5-20250929"
    provider: "anthropic"
    rationale: "Claude Sonnet 4.5 (Jan 2026): SWE-bench verified 77-82%, GPQA Diamond 83.4%, HumanEval >95%. Major upgrade from Claude 3.5 Sonnet with superior logical reasoning and coding capabilities."
    benchmark_data:
      HumanEval: ">95%"
      GPQA_Diamond: "83.4%"
      SWE_bench: "77-82%"
    enabled: true
    fallback: "openai"
    fallback_model: "gpt-5.2"

  pattern:
    model: "gpt-5.2"
    provider: "openai"
    rationale: "Leads abstract reasoning: ARC-AGI-2 (52.9%), MMMU-Pro (86.5%). Surpasses Gemini 3 Pro (31.1%) and Claude Opus 4.5 (37.6%) on pattern recognition. Updated Jan 2026."
    benchmark_data:
      ARC_AGI_2: "52.9%"
      MMMU_Pro: "86.5%"
    enabled: true
    fallback: "anthropic"
    fallback_model: "claude-opus-4-5-20251101"

  spatial:
    model: "gpt-5.2"
    provider: "openai"
    rationale: "Leads spatial reasoning: ARC-AGI-2 (52.9%), MMMU-Pro (86.5%), Video-MMMU (90.5%). Composite 75.9. Fallback upgraded to Gemini 3 Pro (composite 65.5). Updated Jan 2026."
    benchmark_data:
      ARC_AGI_2: "52.9%"
      MMMU_Pro: "86.5%"
      Video_MMMU: "90.5%"
    enabled: true
    fallback: "google"
    fallback_model: "gemini-3-pro-preview"

  verbal:
    model: "claude-sonnet-4-5-20250929"
    provider: "anthropic"
    rationale: "Claude Sonnet 4.5 (Jan 2026): MMLU 89%, HellaSwag ~95%. Enhanced language understanding and generation with superior performance on natural language reasoning tasks."
    benchmark_data:
      MMLU: "89%"
      HellaSwag: "~95%"
    enabled: true
    fallback: "openai"
    fallback_model: "gpt-5.2"

  memory:
    model: "gemini-3-pro-preview"
    provider: "google"
    rationale: "Composite 72.3 leads all models. MMLU 91.8% with 1M token context window. Strong knowledge + largest context for memory-intensive evaluation. Updated Jan 2026."
    benchmark_data:
      MMLU: "91.8%"
      MMLU_Pro: "90.1%"
      context_window: "1,000,000 tokens"
    enabled: true
    fallback: "openai"
    fallback_model: "gpt-5.2"

# Default judge (fallback for unknown question types)
default_judge:
  model: "gpt-4-turbo"
  provider: "openai"
  rationale: "General-purpose fallback judge"
  enabled: true

# Evaluation criteria weights (0.0 to 1.0)
# These determine how heavily each factor is weighted in the ACCEPTANCE score.
# Difficulty is NOT included here - it determines PLACEMENT, not acceptance.
evaluation_criteria:
  clarity: 0.30              # Question is clear and unambiguous
  validity: 0.40             # Valid as an IQ test question
  formatting: 0.20           # Properly formatted with correct answer
  creativity: 0.10           # Novel and interesting question

# Minimum score threshold for question approval (0.0 to 1.0)
min_judge_score: 0.7

# Difficulty placement configuration
# Difficulty score determines where a question is placed, not whether it's accepted.
# A high-quality question that's "too easy for hard" will be accepted but placed at medium.
difficulty_placement:
  # Primary: Use numeric difficulty_score to determine placement
  # If score is below threshold, downgrade one level; if above, upgrade one level
  downgrade_threshold: 0.4   # Score below this → downgrade (e.g., hard → medium)
  upgrade_threshold: 0.8     # Score above this → upgrade (e.g., easy → medium)

  # Fallback: Pattern matching on judge feedback
  # Used when numeric score is ambiguous (between thresholds)
  too_easy_patterns:
    - "too easy"
    - "easier than"
    - "success rate 70"
    - "success rate 80"
    - "straightforward"
  too_hard_patterns:
    - "too hard"
    - "too difficult"
    - "harder than"
    - "success rate 10"
    - "success rate 5"

# Notes:
# - Model identifiers should match the exact model names used by provider SDKs
# - Provider names must match: "openai", "anthropic", "google", or "xai"
# - Enabled flag can be used to temporarily disable specific judges
# - Evaluation criteria weights should sum to 1.0 (difficulty excluded)
