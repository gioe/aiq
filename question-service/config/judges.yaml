# Judge Model Configuration
# Maps question types to specific judge models for quality evaluation
#
# Each question type can be evaluated by a different model based on
# benchmark performance and domain expertise.
#
# Configuration Structure:
#   question_type:
#     model: Exact API model identifier (must match provider SDK expectations)
#     provider: Provider name ("openai", "anthropic", "google", "xai")
#     rationale: Explanation of why this model was chosen
#     enabled: Whether this judge is active (default: true)
#     fallback: Optional fallback provider if primary is unavailable
#     fallback_model: Optional specific model to use with the fallback provider
#
# IMPORTANT - Model Naming Convention:
#   Model identifiers MUST match the exact strings used by each provider's API:
#   - Anthropic: claude-opus-4-5-20251101, claude-sonnet-4-5-20250929, etc.
#   - Google: gemini-3-pro-preview, gemini-3-flash-preview, gemini-2.5-pro, etc.
#   - OpenAI: gpt-5.2, gpt-4-turbo, o4-mini, etc.
#   - xAI: grok-4, grok-3, grok-beta
#   See each provider's get_available_models() for the full list.
#
# Google Model Naming (Preview vs Stable):
#   Google uses "-preview" suffix for new models in preview phase (e.g., gemini-3-pro-preview).
#   Once a model graduates to stable, the suffix is removed (e.g., gemini-2.5-pro).
#   Preview models may have API changes; stable models have frozen APIs.
#   Always check Google's release notes when updating model identifiers.

version: "1.0"

judges:
  math:
    model: "grok-4"
    provider: "xai"
    rationale: "Exceptional math performance: GSM8K (95.2%), AIME 2024 (100%), USAMO 2025 (61.9%). Grok 4 demonstrates world-class mathematical reasoning with perfect scores on advanced math competitions."
    benchmark_data:
      GSM8K: "95.2%"
      AIME_2024: "100%"
      USAMO_2025: "61.9%"
      MMLU: "92.1%"
    enabled: true
    fallback: "anthropic"
    fallback_model: "claude-sonnet-4-5-20250929"
    # Decision Log (Jan 2026): Evaluated Grok 4 Heavy as alternative.
    # Grok 4 Heavy achieves identical USAMO performance (61.9%) but is NOT available
    # via API - only through SuperGrok Heavy subscription ($300/month). The multi-agent
    # architecture adds 4-7x latency overhead with no meaningful improvement for our
    # math judging use case. Keeping grok-4 as optimal choice for API-accessible math judging.

  logic:
    model: "claude-sonnet-4-5-20250929"
    provider: "anthropic"
    rationale: "Claude Sonnet 4.5 (Jan 2026): SWE-bench verified 77-82%, GPQA Diamond 83.4%, HumanEval >95%. Major upgrade from Claude 3.5 Sonnet with superior logical reasoning and coding capabilities."
    benchmark_data:
      HumanEval: ">95%"
      GPQA_Diamond: "83.4%"
      SWE_bench: "77-82%"
    enabled: true
    fallback: "openai"
    fallback_model: "gpt-5.2"

  pattern:
    model: "gemini-3-pro-preview"
    provider: "google"
    rationale: "Superior abstract reasoning: ARC-AGI-2 (31.1%), GPQA Diamond (91.9%). Updated Jan 2026. Using preview identifier per Google API docs."
    benchmark_data:
      ARC_AGI_2: "31.1%"
      GPQA_Diamond: "91.9%"
      MMMU_Pro: "81.0%"
    enabled: true
    fallback: "anthropic"
    fallback_model: "claude-opus-4-5-20251101"

  spatial:
    model: "gemini-3-pro-preview"
    provider: "google"
    rationale: "Breakthrough spatial reasoning: ARC-AGI-2 (31.1% standard mode). Deep Think mode (45.1%) not currently enabled. MMMU-Pro (81.0%). 6x improvement over previous models. Updated Jan 2026. Using preview identifier per Google API docs."
    benchmark_data:
      ARC_AGI_2: "31.1%"
      ARC_AGI_2_Deep_Think: "45.1% (not enabled)"
      MMMU_Pro: "81.0%"
      Visual_Reasoning: "62%"
    enabled: true
    fallback: "anthropic"
    fallback_model: "claude-opus-4-5-20251101"
    # TODO: Evaluate enabling Deep Think mode for spatial judging once API support
    # is available. Deep Think achieves 45.1% on ARC-AGI-2 vs 31.1% standard mode.
    # Considerations: latency impact, cost implications, API availability.

  verbal:
    model: "claude-sonnet-4-5-20250929"
    provider: "anthropic"
    rationale: "Claude Sonnet 4.5 (Jan 2026): MMLU 89%, HellaSwag ~95%. Enhanced language understanding and generation with superior performance on natural language reasoning tasks."
    benchmark_data:
      MMLU: "89%"
      HellaSwag: "~95%"
    enabled: true
    fallback: "openai"
    fallback_model: "gpt-5.2"

  memory:
    model: "claude-sonnet-4-5-20250929"
    provider: "anthropic"
    rationale: "Claude Sonnet 4.5 (Jan 2026): MMLU 89% with 200K token context window. Combines strong knowledge benchmarks with massive context for memory-intensive evaluation tasks."
    benchmark_data:
      MMLU: "89%"
      context_window: "200,000 tokens"
    enabled: true
    fallback: "openai"
    fallback_model: "gpt-5.2"

# Default judge (fallback for unknown question types)
default_judge:
  model: "gpt-4-turbo"
  provider: "openai"
  rationale: "General-purpose fallback judge"
  enabled: true

# Evaluation criteria weights (0.0 to 1.0)
# These determine how heavily each factor is weighted in the ACCEPTANCE score.
# Difficulty is NOT included here - it determines PLACEMENT, not acceptance.
evaluation_criteria:
  clarity: 0.30              # Question is clear and unambiguous
  validity: 0.40             # Valid as an IQ test question
  formatting: 0.20           # Properly formatted with correct answer
  creativity: 0.10           # Novel and interesting question

# Minimum score threshold for question approval (0.0 to 1.0)
min_judge_score: 0.7

# Difficulty placement configuration
# Difficulty score determines where a question is placed, not whether it's accepted.
# A high-quality question that's "too easy for hard" will be accepted but placed at medium.
difficulty_placement:
  # Primary: Use numeric difficulty_score to determine placement
  # If score is below threshold, downgrade one level; if above, upgrade one level
  downgrade_threshold: 0.4   # Score below this → downgrade (e.g., hard → medium)
  upgrade_threshold: 0.8     # Score above this → upgrade (e.g., easy → medium)

  # Fallback: Pattern matching on judge feedback
  # Used when numeric score is ambiguous (between thresholds)
  too_easy_patterns:
    - "too easy"
    - "easier than"
    - "success rate 70"
    - "success rate 80"
    - "straightforward"
  too_hard_patterns:
    - "too hard"
    - "too difficult"
    - "harder than"
    - "success rate 10"
    - "success rate 5"

# Notes:
# - Model identifiers should match the exact model names used by provider SDKs
# - Provider names must match: "openai", "anthropic", "google", or "xai"
# - Enabled flag can be used to temporarily disable specific judges
# - Evaluation criteria weights should sum to 1.0 (difficulty excluded)
