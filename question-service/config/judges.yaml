# Judge Model Configuration
# Maps question types to specific judge models for quality evaluation
#
# Each question type can be evaluated by a different model based on
# benchmark performance and domain expertise.
#
# Configuration Structure:
#   question_type:
#     model: Exact API model identifier (must match provider SDK expectations)
#     provider: Provider name ("openai", "anthropic", "google", "xai")
#     rationale: Explanation of why this model was chosen
#     enabled: Whether this judge is active (default: true)
#
# IMPORTANT - Model Naming Convention:
#   Model identifiers MUST match the exact strings used by each provider's API:
#   - Anthropic: claude-opus-4-5-20251101, claude-sonnet-4-5-20250929, etc.
#   - Google: gemini-3-pro, gemini-3-flash, gemini-2.5-pro, etc.
#   - OpenAI: gpt-5.2, gpt-4-turbo, o4-mini, etc.
#   - xAI: grok-4, grok-3, grok-beta
#   See each provider's get_available_models() for the full list.

version: "1.0"

judges:
  math:
    model: "grok-4"
    provider: "xai"
    rationale: "Exceptional math performance: GSM8K (95.2%), AIME 2024 (100%), USAMO 2025 (61.9%). Grok 4 demonstrates world-class mathematical reasoning with perfect scores on advanced math competitions."
    benchmark_data:
      GSM8K: "95.2%"
      AIME_2024: "100%"
      USAMO_2025: "61.9%"
      MMLU: "92.1%"
    enabled: true
    # Decision Log (Jan 2026): Evaluated Grok 4 Heavy as alternative.
    # Grok 4 Heavy achieves identical USAMO performance (61.9%) but is NOT available
    # via API - only through SuperGrok Heavy subscription ($300/month). The multi-agent
    # architecture adds 4-7x latency overhead with no meaningful improvement for our
    # math judging use case. Keeping grok-4 as optimal choice for API-accessible math judging.

  logic:
    model: "claude-sonnet-4-5-20250929"
    provider: "anthropic"
    rationale: "Claude Sonnet 4.5 (Jan 2026): SWE-bench verified 77-82%, GPQA Diamond 83.4%, HumanEval >95%. Major upgrade from Claude 3.5 Sonnet with superior logical reasoning and coding capabilities."
    benchmark_data:
      HumanEval: ">95%"
      GPQA_Diamond: "83.4%"
      SWE_bench: "77-82%"
    enabled: true

  pattern:
    model: "gemini-3-pro"
    provider: "google"
    rationale: "Superior abstract reasoning: ARC-AGI-2 (31.1%), GPQA Diamond (91.9%). Updated Jan 2026."
    benchmark_data:
      ARC_AGI_2: "31.1%"
      GPQA_Diamond: "91.9%"
      MMMU_Pro: "81.0%"
    enabled: true

  spatial:
    model: "gemini-3-pro"
    provider: "google"
    rationale: "Breakthrough spatial reasoning: ARC-AGI-2 (31.1%, 45.1% Deep Think), MMMU-Pro (81.0%). 6x improvement over previous models. Updated Jan 2026."
    benchmark_data:
      ARC_AGI_2: "31.1%"
      MMMU_Pro: "81.0%"
      Visual_Reasoning: "62%"
    enabled: true

  verbal:
    model: "claude-sonnet-4-5-20250929"
    provider: "anthropic"
    rationale: "Claude Sonnet 4.5 (Jan 2026): MMLU 89%, HellaSwag ~95%. Enhanced language understanding and generation with superior performance on natural language reasoning tasks."
    benchmark_data:
      MMLU: "89%"
      HellaSwag: "~95%"
    enabled: true

  memory:
    model: "claude-sonnet-4-5-20250929"
    provider: "anthropic"
    rationale: "Claude Sonnet 4.5 (Jan 2026): MMLU 89% with 200K token context window. Combines strong knowledge benchmarks with massive context for memory-intensive evaluation tasks."
    benchmark_data:
      MMLU: "89%"
      context_window: "200,000 tokens"
    enabled: true

# Default judge (fallback for unknown question types)
default_judge:
  model: "gpt-4-turbo"
  provider: "openai"
  rationale: "General-purpose fallback judge"
  enabled: true

# Evaluation criteria weights (0.0 to 1.0)
# These determine how heavily each factor is weighted in the judge score
evaluation_criteria:
  clarity: 0.25              # Question is clear and unambiguous
  difficulty: 0.20           # Appropriate difficulty for IQ testing
  validity: 0.30             # Valid as an IQ test question
  formatting: 0.15           # Properly formatted with correct answer
  creativity: 0.10           # Novel and interesting question

# Minimum score threshold for question approval (0.0 to 1.0)
min_judge_score: 0.7

# Notes:
# - Model identifiers should match the exact model names used by provider SDKs
# - Provider names must match: "openai", "anthropic", "google", or "xai"
# - Enabled flag can be used to temporarily disable specific judges
# - Evaluation criteria weights should sum to 1.0
