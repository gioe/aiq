# Judge Model Selection Guide

## Table of Contents
- [Overview](#overview)
- [Current Configuration](#current-configuration)
- [Selection Methodology](#selection-methodology)
- [Benchmark Data Sources](#benchmark-data-sources)
- [Update Process](#update-process)
- [Decision Criteria](#decision-criteria)
- [Change History](#change-history)

---

## Overview

This document explains the research-driven process for selecting specialized judge models for each question type in the IQ Tracker question generation pipeline. Each judge model is chosen based on public benchmark performance in areas relevant to the question type it evaluates.

### Judge Purpose

Judges evaluate candidate questions generated by various LLMs, scoring them on:
- **Clarity** (25%): Question is clear and unambiguous
- **Difficulty** (20%): Appropriate difficulty for IQ testing
- **Validity** (30%): Valid as an IQ test question
- **Formatting** (15%): Properly formatted with correct answer
- **Creativity** (10%): Novel and interesting question

Questions scoring above 0.7 are approved for inclusion in the question pool.

---

## Current Configuration

**Last Updated**: January 2026
**Configuration Version**: 2.0
**Based on**: January 2026 benchmark data

### Question Type Assignments

| Question Type | Judge Model | Provider | Key Benchmarks |
|--------------|---------------|----------|----------------|
| **mathematical** | grok-4 | xAI | GSM8K: 95.2%, AIME 2024: 100%, USAMO 2025: 61.9% |
| **logical_reasoning** | claude-sonnet-4-5-20250929 | Anthropic | HumanEval: >95%, GPQA Diamond: 83.4%, SWE-bench: 77-82% |
| **pattern_recognition** | gemini-3-pro-preview | Google | ARC-AGI-2: 31.1%, GPQA Diamond: 91.9%, MMMU-Pro: 81.0% |
| **spatial_reasoning** | gemini-3-pro-preview | Google | ARC-AGI-2: 31.1% (45.1% Deep Think), MMMU-Pro: 81.0% |
| **verbal_reasoning** | claude-sonnet-4-5-20250929 | Anthropic | MMLU: 89%, HellaSwag: ~95% |
| **memory** | claude-sonnet-4-5-20250929 | Anthropic | MMLU: 89%, 200K context |
| **default** | gpt-4-turbo | OpenAI | General-purpose fallback |

### Key Findings (January 2026)

**Claude Sonnet 4.5** leads logical/verbal/memory question types:
- Major upgrade from Claude 3.5 Sonnet with SWE-bench improving from 49% to 77-82%
- Exceptional reasoning performance (GPQA Diamond: 83.4%, HumanEval: >95%)
- Strong knowledge retention (MMLU: 89%)
- Massive context window (200K tokens) beneficial for memory tasks

**Gemini 3 Pro** breakthrough for pattern/spatial reasoning:
- ARC-AGI-2: 31.1% (6x improvement over previous models)
- Deep Think mode achieves 45.1% on ARC-AGI-2
- MMMU-Pro: 81.0% multimodal reasoning
- GPQA Diamond: 91.9% (highest among all models)

**Grok 4 (xAI)** dominates mathematical reasoning:
- Perfect score on AIME 2024 (100%)
- USAMO 2025: 61.9% (competition-level mathematics)
- GSM8K: 95.2% (slightly above GPT-4 Turbo's 95.0%)

**GPT-4 Turbo retained** for:
- Default fallback (broad competence across tasks)

---

## Selection Methodology

### Question Type → Benchmark Mapping

Each question type was mapped to relevant public benchmarks:

#### 1. Mathematical
**Relevant Benchmarks**:
- **GSM8K** (Grade School Math 8K): Elementary to middle-school level math word problems
- **MATH 500**: Competition-level mathematics subset
- **AIME 2024/2025**: American Invitational Mathematics Examination
- **USAMO 2025**: USA Mathematical Olympiad

**Decision Logic**: Mathematical questions require strong quantitative reasoning. GSM8K tests basic problem-solving, while AIME/USAMO evaluate competition-level mathematical thinking.

**Winner**: Grok 4 (xAI) - GSM8K: 95.2%, AIME 2024: 100%, USAMO 2025: 61.9%

#### 2. Logical Reasoning
**Relevant Benchmarks**:
- **HumanEval**: Coding problems requiring logical decomposition
- **GPQA Diamond**: Graduate-Level Google-Proof Q&A (hardest subset)
- **SWE-bench Verified**: Real-world software engineering tasks

**Decision Logic**: Logical reasoning requires systematic thinking, inference, and deductive capabilities tested by coding and graduate-level problems.

**Winner**: Claude Sonnet 4.5 - HumanEval: >95%, GPQA Diamond: 83.4%, SWE-bench: 77-82%

#### 3. Pattern Recognition
**Relevant Benchmarks**:
- **ARC-AGI-2**: Abstract Reasoning Corpus (next-generation, harder than original)
- **GPQA Diamond**: Abstract reasoning patterns (hardest subset)
- **MMMU-Pro**: Multimodal understanding with pattern recognition

**Decision Logic**: Pattern recognition requires identifying abstract relationships and structures. ARC-AGI-2 specifically tests novel pattern completion, making it the gold standard for this domain.

**Winner**: Gemini 3 Pro - ARC-AGI-2: 31.1%, GPQA Diamond: 91.9%, MMMU-Pro: 81.0%

#### 4. Spatial Reasoning
**Relevant Benchmarks**:
- **ARC-AGI-2**: Abstract visual-spatial reasoning tasks
- **ARC-AGI-2 Deep Think**: Extended reasoning mode for complex spatial tasks
- **MMMU-Pro**: Multimodal understanding including spatial components

**Decision Logic**: ARC-AGI-2 provides the first rigorous benchmark for spatial reasoning. Gemini 3 Pro's breakthrough 31.1% score (45.1% with Deep Think) represents 6x improvement over previous models.

**Winner**: Gemini 3 Pro - ARC-AGI-2: 31.1% (45.1% Deep Think), MMMU-Pro: 81.0%

#### 5. Verbal Reasoning
**Relevant Benchmarks**:
- **MMLU**: Comprehensive language understanding across 57 subjects
- **HellaSwag**: Commonsense language reasoning
- **WinoGrande**: Coreference resolution and language understanding

**Decision Logic**: Verbal reasoning requires nuanced language understanding, vocabulary, and comprehension across diverse contexts.

**Winner**: Claude Sonnet 4.5 - MMLU: 89%, HellaSwag: ~95%

#### 6. Memory
**Relevant Benchmarks**:
- **MMLU**: Knowledge recall and retention
- **Context window size**: Proxy for information retention capability

**Decision Logic**: Memory questions test recall and retention. MMLU measures factual knowledge, while large context windows indicate better information management.

**Winner**: Claude Sonnet 4.5 - MMLU: 89%, 200K token context window

---

## Benchmark Data Sources

### Primary Benchmarks

#### MMLU (Massive Multitask Language Understanding)
- **Description**: 57 subjects spanning STEM, humanities, social sciences
- **Format**: Multiple-choice questions
- **Link**: https://github.com/hendrycks/test

#### GSM8K (Grade School Math 8K)
- **Description**: 8,500 grade school math word problems
- **Format**: Free-form numerical answers
- **Link**: https://github.com/openai/grade-school-math

#### MATH
- **Description**: 12,500 competition mathematics problems
- **Difficulty**: High school to undergraduate level
- **Link**: https://github.com/hendrycks/math

#### HumanEval
- **Description**: 164 programming problems
- **Format**: Function implementation tasks
- **Link**: https://github.com/openai/human-eval

#### GPQA Diamond (Graduate-Level Google-Proof Q&A)
- **Description**: Hardest subset of GPQA - graduate-level reasoning questions resistant to search
- **Difficulty**: PhD-level across biology, physics, chemistry
- **Link**: Research benchmark (published 2023, Diamond subset most commonly referenced)

#### ARC-AGI-2 (Abstract Reasoning Corpus - Next Generation)
- **Description**: Next-generation abstract reasoning benchmark testing novel pattern completion
- **Format**: Visual grid-based pattern completion tasks
- **Difficulty**: Designed to test general intelligence, not just memorized patterns
- **Link**: https://arcprize.org/

#### SWE-bench Verified
- **Description**: Real-world software engineering tasks from GitHub issues
- **Format**: Fixing actual bugs and implementing features in real codebases
- **Difficulty**: Professional-level software engineering
- **Link**: https://www.swebench.com/

#### MMMU-Pro (Massive Multi-discipline Multimodal Understanding - Pro)
- **Description**: Harder multimodal reasoning across diverse academic disciplines
- **Format**: Vision-language tasks requiring deep understanding
- **Link**: Research benchmark (2024-2025)

#### AIME (American Invitational Mathematics Examination)
- **Description**: Competition mathematics for advanced high school students
- **Difficulty**: Top 2.5% of AMC participants
- **Format**: 15 problems, answers are integers 000-999
- **Link**: https://artofproblemsolving.com/

#### USAMO (USA Mathematical Olympiad)
- **Description**: Most prestigious US high school math competition
- **Difficulty**: Proof-based problems requiring deep mathematical insight
- **Format**: 6 problems over 2 days, 9 hours total
- **Link**: https://artofproblemsolving.com/

#### HellaSwag
- **Description**: Commonsense natural language inference
- **Format**: Sentence completion tasks
- **Link**: https://rowanzellers.com/hellaswag/

#### WinoGrande
- **Description**: Commonsense reasoning (Winograd Schema Challenge)
- **Format**: Pronoun resolution tasks
- **Link**: https://winogrande.allenai.org/

### Leaderboards & Resources

- **Vellum LLM Leaderboard**: https://www.vellum.ai/llm-leaderboard
- **Papers with Code**: https://paperswithcode.com/
- **Hugging Face Open LLM Leaderboard**: https://huggingface.co/spaces/open-llm-leaderboard
- **Official model announcements**: OpenAI, Anthropic, Google blogs
- **Academic papers**: Model technical reports and research publications

### January 2026 Benchmark Scores (Reference)

**Claude Sonnet 4.5 (claude-sonnet-4-5-20250929)**:
- MMLU: 89%
- HumanEval: >95%
- GPQA Diamond: 83.4%
- SWE-bench Verified: 77-82%
- HellaSwag: ~95%
- Context Window: 200K tokens

**Claude Opus 4.5 (claude-opus-4-5-20251101)**:
- GPQA Diamond: ~87%
- SWE-bench Verified: 82-85%
- HumanEval: >95%
- MMLU: ~91%

**Grok 4 (xAI)**:
- GSM8K: 95.2%
- AIME 2024: 100%
- USAMO 2025: 61.9%
- MMLU: 92.1%

**Gemini 3 Pro (gemini-3-pro-preview)**:
- ARC-AGI-2: 31.1% (45.1% Deep Think)
- GPQA Diamond: 91.9%
- MMMU-Pro: 81.0%
- MMLU: ~92%

**GPT-5.2 (OpenAI)**:
- GSM8K: ~96%
- MATH 500: ~92%
- AIME 2025: ~85%
- HumanEval: ~95%

**o4-mini (OpenAI)**:
- AIME 2024: 93.4%
- MATH 500: 97.3%
- Cost-effective for math evaluation

### Previous Benchmark Scores (2024 Reference)

**Claude 3.5 Sonnet (claude-3-5-sonnet-20241022)**:
- MMLU: 90.4%
- HumanEval: 93.7%
- GPQA: 67.2%
- SWE-bench: 49%
- GSM8K: ~95%

**GPT-4 Turbo**:
- MMLU: ~88.7%
- GSM8K: 95.0%
- MATH: 60.1%
- HumanEval: ~90%

**Gemini 1.5 Pro**:
- MMLU: ~88.6%
- HellaSwag: 93.3%
- HumanEval: 71.9%

---

## Update Process

### Update Schedule

**Recommended Frequency**: **Quarterly** (every 3 months)

**Rationale**:
- Major model releases: 2-4 times per year
- Benchmark updates: Continuous, but meaningful changes occur quarterly
- Question quality: Won't degrade rapidly with slightly outdated judges
- Operational balance: Staying current without excessive overhead

### When to Update

Perform a configuration review when:
1. **Scheduled quarterly review** (March, June, September, December)
2. **Major model release** (GPT-5, Claude 4, Gemini 2.0, etc.)
3. **New benchmark publication** that significantly changes rankings
4. **Production quality issues** traced to judge performance

### Update Workflow

#### Step 1: Gather Latest Benchmark Data

```bash
# Visit benchmark leaderboards
1. Vellum LLM Leaderboard: https://www.vellum.ai/llm-leaderboard
2. Hugging Face Open LLM Leaderboard
3. Official model documentation (OpenAI, Anthropic, Google)
4. Papers with Code: https://paperswithcode.com/
```

Create a spreadsheet or table with current scores:

| Model | MMLU | GSM8K | MATH | HumanEval | GPQA | HellaSwag |
|-------|------|-------|------|-----------|------|-----------|
| GPT-4 Turbo | | | | | | |
| Claude 3.5 Sonnet | | | | | | |
| Gemini Pro | | | | | | |
| ... | | | | | | |

#### Step 2: Analyze Performance Changes

For each question type:
1. Identify the current judge and its benchmark scores
2. Check if any alternative model now significantly outperforms it
3. Consider **switching threshold**: >5% improvement on primary benchmark

Example:
```
Current: claude-3-5-sonnet (GPQA: 67.2%)
Alternative: hypothetical-model-x (GPQA: 73.0%)
Improvement: +5.8% → CONSIDER SWITCHING
```

#### Step 3: Update Configuration File

If switching is warranted:

```bash
cd question-service
nano config/judges.yaml  # or use your preferred editor
```

Update the relevant section:
```yaml
logical_reasoning:
  model: "new-model-name"
  provider: "provider-name"
  rationale: "Updated reasoning: [benchmark scores]. Previous: claude-3-5-sonnet (GPQA: 67.2%). New model shows significant improvement..."
  benchmark_data:
    HumanEval: "XX.X%"
    GPQA: "XX.X%"
  enabled: true
```

#### Step 4: Document the Change

Add entry to this document's [Change History](#change-history) section:

```markdown
### [Date] - Configuration Update

**Changed**: [question_type] judge: [old_model] → [new_model]

**Reason**: New model shows [X]% improvement on [benchmark_name]

**Benchmark Comparison**:
- Old: [benchmark scores]
- New: [benchmark scores]

**Impact**: Expected improvement in [aspect of question evaluation]
```

#### Step 5: Test Configuration

```bash
cd question-service
source venv/bin/activate
python -m pytest tests/  # Ensure tests pass
python examples/judge_config_example.py  # Verify config loads correctly
```

#### Step 6: Deploy Update

```bash
git add config/judges.yaml docs/JUDGE_SELECTION.md
git commit -m "[JUDGE] Update judge configuration - [date]"
git push origin main
```

If question service is already deployed:
```bash
# Restart the service to pick up new configuration
# (Specific commands depend on deployment method)
```

#### Step 7: Monitor Question Quality

After deployment:
- Review generated questions for 1-2 weeks
- Check judge approval rates
- Compare question quality subjectively
- Revert if quality degrades

---

## Decision Criteria

### When to Switch Judge Models

Use the following criteria to decide if an judge change is warranted:

#### Primary Criteria: Benchmark Performance
- **Threshold**: >5% improvement on primary benchmark(s)
- **Multiple benchmarks**: Improvement across multiple relevant benchmarks
- **Consistency**: Performance maintained across diverse tasks in the domain

#### Secondary Considerations

1. **Cost**: New model's API pricing vs. current model
   - Only switch if cost increase is <20% OR performance gain >10%

2. **Latency**: Response time for judge evaluations
   - Avoid models >2x slower unless performance gain is substantial (>15%)

3. **API Reliability**: Uptime and rate limits of provider
   - Ensure provider meets our SLA requirements

4. **Model Availability**: Stable API access
   - Prefer generally available models over limited beta access

5. **Context Window**: Sufficient for question + evaluation prompt
   - Minimum 4K tokens required; prefer 8K+

#### Special Cases

**Gemini 3 Pro**: Now selected for pattern and spatial reasoning due to:
- Breakthrough ARC-AGI-2 performance (31.1%, 6x improvement over previous models)
- Highest GPQA Diamond score (91.9%) among all models
- Deep Think mode provides additional reasoning power (45.1% on ARC-AGI-2)

**Grok 4 (xAI)**: Selected for mathematical reasoning due to:
- Perfect AIME 2024 score (100%)
- Strong USAMO 2025 performance (61.9%)
- Competitive GSM8K (95.2%, slightly above GPT-4 Turbo)

**Claude Sonnet 4.5 vs Opus 4.5**:
- Opus 4.5 shows higher scores on some benchmarks (GPQA Diamond ~87% vs 83.4%)
- Sonnet 4.5 selected for cost-efficiency with excellent performance
- Consider Opus 4.5 for mission-critical evaluations if budget allows

**GPT-5.2 Consideration**:
- Strong math performance but Grok 4's AIME/USAMO scores are more impressive
- Retaining GPT-4 Turbo as default fallback for broad competence

#### Risk Assessment

Before switching, consider:
- **Reversibility**: Can we easily revert if quality degrades?
- **A/B Testing**: Can we test new judge on subset before full rollout?
- **Backup Configuration**: Maintain previous config for quick rollback

---

## Decision Criteria Summary

### Switching Checklist

Use this checklist when evaluating judge changes:

- [ ] New model shows >5% improvement on primary benchmark
- [ ] Performance validated across multiple relevant benchmarks
- [ ] Cost increase is acceptable (<20%) OR performance gain is substantial (>10%)
- [ ] Latency is acceptable (<2x current model)
- [ ] Provider API is reliable and meets SLA
- [ ] Model is generally available (not limited beta)
- [ ] Context window sufficient (8K+ tokens preferred)
- [ ] Configuration updated in `config/judges.yaml`
- [ ] Change documented in this file's Change History
- [ ] Tests pass with new configuration
- [ ] Monitoring plan in place for post-deployment quality checks

---

## Change History

### 2026-01-24 - January 2026 Quarterly Review (v2.0)

**Review Type**: Quarterly review (Q1 2026) - Missed April 2025 review, now 12+ months since last update

**Major Model Changes**:

1. **mathematical**: gpt-4-turbo → grok-4 (xAI)
   - Reason: Grok 4 achieves perfect AIME 2024 (100%), USAMO 2025 (61.9%)
   - Previous: GPT-4 Turbo (GSM8K: 95.0%, MATH: 60.1%)
   - New: Grok 4 (GSM8K: 95.2%, AIME 2024: 100%, USAMO 2025: 61.9%)
   - Improvement: Competition-level math performance far exceeds previous judge

2. **logical_reasoning**: claude-3-5-sonnet-20241022 → claude-sonnet-4-5-20250929
   - Reason: Major Claude upgrade with SWE-bench improving from 49% to 77-82%
   - Previous: Claude 3.5 Sonnet (HumanEval: 93.7%, GPQA: 67.2%, SWE-bench: 49%)
   - New: Claude Sonnet 4.5 (HumanEval: >95%, GPQA Diamond: 83.4%, SWE-bench: 77-82%)
   - Improvement: +16 points on GPQA Diamond, +28-33 points on SWE-bench

3. **pattern_recognition**: claude-3-5-sonnet-20241022 → gemini-3-pro-preview
   - Reason: Gemini 3 Pro breakthrough on ARC-AGI-2 (31.1%, 6x improvement)
   - Previous: Claude 3.5 Sonnet (GPQA: 67.2%, MMLU: 90.4%)
   - New: Gemini 3 Pro (ARC-AGI-2: 31.1%, GPQA Diamond: 91.9%, MMMU-Pro: 81.0%)
   - Improvement: First model to significantly advance on abstract reasoning benchmarks

4. **spatial_reasoning**: claude-3-5-sonnet-20241022 → gemini-3-pro-preview
   - Reason: ARC-AGI-2 provides first rigorous spatial reasoning benchmark
   - Previous: Claude 3.5 Sonnet (no dedicated spatial benchmark available)
   - New: Gemini 3 Pro (ARC-AGI-2: 31.1%, Deep Think: 45.1%)
   - Improvement: Now using purpose-built spatial reasoning evaluation

5. **verbal_reasoning**: claude-3-5-sonnet-20241022 → claude-sonnet-4-5-20250929
   - Reason: Claude Sonnet 4.5 upgrade with improved language understanding
   - Previous: Claude 3.5 Sonnet (MMLU: 90.4%, HellaSwag: ~90%)
   - New: Claude Sonnet 4.5 (MMLU: 89%, HellaSwag: ~95%)
   - Note: Slight MMLU decrease but improved HellaSwag and overall capability

6. **memory**: claude-3-5-sonnet-20241022 → claude-sonnet-4-5-20250929
   - Reason: Claude Sonnet 4.5 upgrade, retains 200K context window
   - Previous: Claude 3.5 Sonnet (MMLU: 90.4%)
   - New: Claude Sonnet 4.5 (MMLU: 89%, 200K context)

**New Benchmarks Added**:
- ARC-AGI-2: Abstract reasoning (replaces limited spatial-specific benchmarks)
- SWE-bench Verified: Real-world software engineering
- AIME 2024/2025: Competition mathematics
- USAMO 2025: Olympiad-level mathematics
- MMMU-Pro: Multimodal understanding

**Configuration Files Updated**:
- `config/judges.yaml`: All judge assignments updated
- `config/generators.yaml`: Generator assignments aligned with judges

**Research Sources**:
- Official model announcements (Anthropic, Google, xAI, OpenAI)
- ARC Prize 2025 leaderboard
- SWE-bench official results
- Academic benchmark publications

**Next Review**: April 2026 (Q2 2026 quarterly review)

---

### 2025-01-11 - Initial Configuration (v1.0)

**Created**: Evidence-based judge configuration based on 2024 benchmark research

**Model Assignments**:
- **mathematical**: gpt-4-turbo (OpenAI)
- **logical_reasoning**: claude-3-5-sonnet-20241022 (Anthropic)
- **pattern_recognition**: claude-3-5-sonnet-20241022 (Anthropic) - *Changed from gpt-4-turbo*
- **spatial_reasoning**: claude-3-5-sonnet-20241022 (Anthropic)
- **verbal_reasoning**: claude-3-5-sonnet-20241022 (Anthropic)
- **memory**: claude-3-5-sonnet-20241022 (Anthropic) - *Changed from gpt-4-turbo*

**Key Changes from Placeholder Config**:
1. Switched `pattern_recognition` from GPT-4 Turbo to Claude 3.5 Sonnet
   - Reason: Superior GPQA performance (67.2% vs 35.7% for GPT-4)
   - Expected improvement in abstract reasoning evaluation

2. Switched `memory` from GPT-4 Turbo to Claude 3.5 Sonnet
   - Reason: Higher MMLU score (90.4% vs ~88%) + massive 200K context window
   - Expected improvement in evaluating recall-based questions

**Research Sources**:
- Vellum LLM Leaderboard (2024)
- Official model documentation (OpenAI, Anthropic, Google)
- Academic benchmark publications (GPQA, MMLU, GSM8K, MATH, HumanEval)

**Next Review**: ~~April 2025 (Q2 2025 quarterly review)~~ *Missed - completed January 2026*

---

## Appendix: Benchmark Descriptions

### Detailed Benchmark Information

#### MMLU (Massive Multitask Language Understanding)
- **Size**: 15,908 questions across 57 subjects
- **Subjects**: Abstract algebra, US history, professional medicine, elementary mathematics, etc.
- **Format**: 4-choice multiple choice
- **Evaluation**: Zero-shot and few-shot accuracy
- **Why it matters**: Tests broad knowledge and reasoning across all cognitive domains

#### GSM8K (Grade School Math 8K)
- **Size**: 8,500 problems
- **Difficulty**: Elementary and middle school level
- **Format**: Math word problems with numerical answers
- **Why it matters**: Tests basic quantitative reasoning and problem decomposition

#### MATH
- **Size**: 12,500 problems
- **Difficulty**: Competition mathematics (AMC 10/12, AIME, etc.)
- **Format**: Free-form answers requiring symbolic math
- **Why it matters**: Tests advanced mathematical reasoning beyond basic arithmetic

#### HumanEval
- **Size**: 164 programming problems
- **Format**: Python function implementation
- **Evaluation**: Functional correctness via unit tests
- **Why it matters**: Proxy for logical reasoning, decomposition, and systematic thinking

#### GPQA (Graduate-Level Google-Proof Q&A)
- **Size**: Multiple sets (Diamond: 198 questions)
- **Difficulty**: PhD-level questions designed to be non-searchable
- **Domains**: Biology, physics, chemistry, and more
- **Why it matters**: Tests highest-level reasoning and domain expertise

#### HellaSwag
- **Size**: 70,000 scenarios
- **Format**: Sentence completion with 4 choices
- **Focus**: Commonsense natural language inference
- **Why it matters**: Tests intuitive understanding of everyday situations

#### WinoGrande
- **Size**: 44,000 problems
- **Format**: Fill-in-the-blank pronoun resolution
- **Focus**: Commonsense reasoning
- **Why it matters**: Tests subtle language understanding and context reasoning

---

## Contact & Questions

For questions about judge configuration or to report issues:
- **GitHub Issues**: [Link to repository issues]
- **Configuration Issues**: Check `config/judges.yaml` format and model availability
- **Benchmark Questions**: Refer to benchmark documentation linked in this guide

**Maintainers**: Question generation service team
**Last Updated**: January 24, 2026
